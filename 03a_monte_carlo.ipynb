{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environments import OneStateEnv, GridWorldEnv\n",
    "from agents.monte_carlo import OnPolicy_Agent, OffPolicy_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib styles\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (18, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for playing until terminal state\n",
    "def play(env, agent): \n",
    "    episode = []\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        a = agent.get_action(s)\n",
    "        t, r, done, _ = env.step(a)\n",
    "        \n",
    "        episode.append([s, a, r, t])\n",
    "        s = t\n",
    "    else:\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates avg. episodes lengths and avg. max updated over given training trials\n",
    "def evaluate(env, agent, trials = 50, episodes = 1000):\n",
    "    total_episode_lengths = []\n",
    "    max_episode_updates = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "        episode_lengths = []\n",
    "        agent.reset()\n",
    "\n",
    "        for _ in range(episodes):\n",
    "            episode = play(env, agent)\n",
    "            agent.learn(episode)\n",
    "            \n",
    "            if agent.__class__.__name__ == 'OffPolicy_Agent':\n",
    "                agent.is_learning = False\n",
    "                episode = play(env, agent)\n",
    "                episode_lengths.append(len(episode))\n",
    "                agent.is_learning = True\n",
    "            else:\n",
    "                episode_lengths.append(len(episode))\n",
    "                \n",
    "        \n",
    "        total_episode_lengths.append(episode_lengths)\n",
    "        max_episode_updates.append(agent.max_updates)\n",
    "\n",
    "    return (np.mean(np.array(total_episode_lengths), axis = 0), np.mean(np.array(max_episode_updates), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting agent episode lengths and max updates\n",
    "def plot_agents(history, connected = False):\n",
    "    if not connected:\n",
    "        fig, axs = plt.subplots(len(history), 2)\n",
    "\n",
    "        handles = []\n",
    "        labels = []\n",
    "\n",
    "        axs[0, 0].set_title('Avg. Steps per Episode\\n')\n",
    "        axs[0, 1].set_title('Avg. max Update per Episode\\n')\n",
    "\n",
    "        row = 0\n",
    "        for agent in history.items():\n",
    "            name = agent[0]\n",
    "            values = agent[1]\n",
    "            labels.append(agent[0])\n",
    "\n",
    "            col = 0\n",
    "            for value in values.items():\n",
    "                handles.append(axs[row, col].plot(value[1], color = f'C{row}', label = name))\n",
    "                col += 1\n",
    "            \n",
    "            row += 1\n",
    "\n",
    "        plt.legend(handles = list([l[0] for l in handles])[::2], labels = labels, loc = 'lower center', ncol = len(history), bbox_to_anchor = (-0.1, -0.6))\n",
    "        plt.show(fig);\n",
    "    \n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "        for agent, values in history.items():\n",
    "            ax1.plot(values['lengths'], label = agent)\n",
    "\n",
    "        ax1.set(xlabel = 'Episodes', ylabel = 'Avg. Steps per Episode')\n",
    "        ax1.legend(loc = 'best')\n",
    "\n",
    "        for agent, values in history.items():\n",
    "            ax2.plot(values['updates'], label = agent)\n",
    "\n",
    "        ax2.set(xlabel = 'Episodes', ylabel = 'Avg. max Update per Episode')\n",
    "        ax2.legend(loc = 'best')\n",
    "\n",
    "        plt.show(fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = GridWorldEnv()\n",
    "\n",
    "# history dict to store avg. episode lengths and avg. max updates of evaluated agents\n",
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate every-visit on-policy agent\n",
    "agent = OnPolicy_Agent(state_space = env.state_space, action_space = env.action_space, epsilon = 0.1, first_visit = False)\n",
    "lengths, updates = evaluate(env = env, agent = agent)\n",
    "\n",
    "history.update({\n",
    "    'Every-Visit On-Policy' : {\n",
    "        'lengths' : lengths,\n",
    "        'updates' : updates\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate first-visit on-policy agent\n",
    "agent = OnPolicy_Agent(state_space = env.state_space, action_space = env.action_space, epsilon = 0.1, first_visit = True)\n",
    "lengths, updates = evaluate(env = env, agent = agent)\n",
    "\n",
    "history.update({\n",
    "    'First-Visit On-Policy' : {\n",
    "        'lengths' : lengths,\n",
    "        'updates' : updates\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# evaluate every-visit off-policy agent w/ weighted importance sampling\n",
    "agent = OffPolicy_Agent(state_space = env.state_space, action_space = env.action_space, epsilon = 0.1, first_visit = False)\n",
    "lengths, updates = evaluate(env = env, agent = agent)\n",
    "\n",
    "history.update({\n",
    "    'Every-Visit Off-Policy' : {\n",
    "        'lengths' : lengths,\n",
    "        'updates' : updates\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "# evaluate first-visit off-policy agent w/ weighted importance sampling\n",
    "agent = OffPolicy_Agent(state_space = env.state_space, action_space = env.action_space, epsilon = 0.1, first_visit = True)\n",
    "lengths, updates = evaluate(env = env, agent = agent)\n",
    "\n",
    "history.update({\n",
    "    'First-Visit Off-Policy' : {\n",
    "        'lengths' : lengths,\n",
    "        'updates' : updates\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show avg. steps per episode and avg. max updates for all agents combined\n",
    "plot_agents(history, connected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show avg. steps per episode and avg. max updates for each agent separately\n",
    "plot_agents(history, connected = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1d093f6a6dbbbf7ba25abe0fb2804f9a5b69bb101e9164c56b05d12f44a29a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
